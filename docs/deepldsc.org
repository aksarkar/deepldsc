#+TITLE: deepldsc
#+AUTHOR: Abhishek Sarkar
#+EMAIL: aksarkar@uchicago.edu
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.1 (Org mode 9.1.1)
* Setup :noexport:

  #+BEGIN_SRC emacs-lisp
    (setq python-shell-prompt-detect-failure-warning nil)
  #+END_SRC

  #+RESULTS:

  #+NAME: ipython3-kernel
  #+BEGIN_SRC shell :dir (concat (file-name-as-directory (getenv "SCRATCH")) "deepldsc") :var RESOURCES="--mem=36G --partition=gpu2 --gres=gpu:1"
    sbatch $RESOURCES --job-name=ipython3 --output=ipython3.out
    #!/bin/bash -l
    source activate deepldsc
    rm -f $HOME/.local/share/jupyter/runtime/kernel-aksarkar.json
    ipython3 kernel --ip=$(hostname -i) -f kernel-aksarkar.json
  #+END_SRC

  #+RESULTS: ipython3-kernel
  : Submitted batch job 38386410

  #+NAME: imports
  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer
    %matplotlib inline
    import deeplift.conversion.keras_conversion as kc
    import keras
    import matplotlib.pyplot as plt
    import numpy as np
    import os
    import pandas as pd
    import sklearn.manifold
    import tensorflow as tf
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  :END:

* Get the data

  #+BEGIN_SRC shell :dir $SCRATCH/deepldsc
  wget -r -l 1 -A "*.sumstats" "https://data.broadinstitute.org/alkesgroup/sumstats_formatted/"
  curl -sfS "https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase1_baseline_ldscores.tgz" | tar xz
  curl -sfS "https://data.broadinstitute.org/alkesgroup/LDSCORE/1000G_Phase1_cell_type_ldscores.tgz" | tar xz
  #+END_SRC

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    uc = pd.read_table('/scratch/midway2/aksarkar/deepldsc/data.broadinstitute.org/alkesgroup/sumstats_formatted/PASS_Ulcerative_Colitis.sumstats')

    # Hold out chr22
    training_data = pd.concat(
        uc.merge(chunk, on='SNP') for chrom in range(1, 22)
        for chunk in pd.read_table('/scratch/midway2/aksarkar/deepldsc/baseline/baseline.{}.l2.ldscore.gz'.format(chrom), chunksize=1000)
    )
    test_data = pd.concat(
        uc.merge(chunk, on='SNP') for chrom in range(22, 23)
        for chunk in pd.read_table('/scratch/midway2/aksarkar/deepldsc/baseline/baseline.{}.l2.ldscore.gz'.format(chrom), chunksize=1000)
    )
    training_data.shape, test_data.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : ((967394, 63), (13897, 63))
  :END:

* deepldsc versus ldsc

  ~ldsc~ is a linear model:

  \[ E[\Chi^2] = N \sum_c \tau_c l(j, c) + N a + 1 \]

  \[ y \sim N(X w + b, \sigma^2) \]

  #+NAME: ldsc
  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    ldsc = keras.models.Sequential([
      keras.layers.Dense(1, input_dim=53)
    ])
    ldsc.compile(optimizer='rmsprop', loss='mse')
    ldsc.fit(x=training_data.iloc[:,10:].values,
             y=training_data['CHISQ'].values,
             epochs=4,
    )
    ldsc.save(os.path.join(os.getenv('SCRATCH'), 'deepldsc', 'uc-ldsc.hdf5'))
    ldsc.evaluate(x=test_data.iloc[:,10:].values,
                  y=test_data['CHISQ'].values)
  #+END_SRC

  #+RESULTS: ldsc
  :RESULTS:
  : 8.3588527845651832
  :END:

  Let's try to fit a deep network instead. The idea is that the architecture of
  the deep network can automatically learn and account for complicated
  correlations between the different features.

  \[ y \sim N(h(X), \sigma^2) \]

  #+NAME: deepldsc
  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    deepldsc = keras.models.Sequential([
      keras.layers.Dense(32, input_dim=53, activation='relu'),
      keras.layers.Dense(16, activation='relu'),
      keras.layers.Dense(1)
    ])
    deepldsc.compile(optimizer=keras.optimizers.RMSprop(lr=1e-4),
                     loss='mse')
    deepldsc.fit(x=training_data.iloc[:,10:].values,
                 y=training_data['CHISQ'].values,
                 epochs=4,
    )
    deepldsc.save(os.path.join(os.getenv('SCRATCH'), 'deepldsc', 'uc-deepldsc.hdf5'))
    deepldsc.evaluate(x=test_data.iloc[:,10:].values,
                      y=test_data['CHISQ'].values)
  #+END_SRC

  #+RESULTS: deepldsc
  :RESULTS:
  : 6.7853382878434942
  :END:

* Interpretation using deeplift

  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    deepldsc = keras.models.load_model(os.path.join(os.getenv('SCRATCH'), 'deepldsc', 'uc-deepldsc.hdf5'))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+NAME: deeplift
  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    scores = kc.convert_sequential_model(deepldsc).get_target_contribs_func(find_scores_layer_idx=0, target_layer_idx=-1)(
      task_idx=0,
      input_data_list=[test_data.iloc[:,10:]],
      batch_size=32,
      progress_update=1000,
    )
    scores = pd.DataFrame(scores)
    scores.columns = test_data.columns[10:]
    scores.index = test_data['SNP']
  #+END_SRC

  #+RESULTS: deeplift
  :RESULTS:
  :END:

  Unlike ~ldsc~, ~deeplift~ on ~deepldsc~ propagates down to each SNP which
  means we can do things like look for heterogeneity among SNPs.

  #+NAME: tsne
  #+BEGIN_SRC ipython :ipyfile tsne.svg :session kernel-aksarkar.json :results raw drawer :async t
    embedding = sklearn.manifold.TSNE().fit_transform(scores)
    plt.clf()
    plt.gcf().set_size_inches(6, 6)
    plt.plot(embedding[:,0], embedding[:,1])
  #+END_SRC

  #+RESULTS: tsne
  :RESULTS:
  : [<matplotlib.lines.Line2D at 0x7f01cddb0748>]
  [[file:tsne.svg]]
  :END:

* Multi-task deepldsc

  The idea of the ~deepldsc~ architecture is to perform dimensionality
  reduction on the features to automatically account for complicated
  correlations between them.

  We can do the same on the phenotypes, and hopefully improve performance on a
  multi-task problem of predicting the \(\Chi^2\) statistics of all of the
  phenotypes simultaneously.

  The only existing method is cross-trait ~ldsc~, which is actually a
  univariate regression:

  \[E[z_1 z_2] = \frac{\sqrt{N_1 N_2} \rho}{M} l_j + \mathrm{const}]

  *TODO:* Nobody's actually done partitioned cross-trait ~ldsc~ yet?

  #+NAME: bivariate-sumstats
  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    import functools
    import glob

    ibd = pd.read_table('/scratch/midway2/aksarkar/deepldsc/data.broadinstitute.org/alkesgroup/sumstats_formatted/PASS_IBD.sumstats')
    bivariate = uc.merge(ibd, on='SNP')

    # Hold out chr22
    training_data = pd.concat(
        bivariate.merge(chunk, on='SNP') for chrom in range(1, 22)
        for chunk in pd.read_table('/scratch/midway2/aksarkar/deepldsc/baseline/baseline.{}.l2.ldscore.gz'.format(chrom), chunksize=1000)
    )
    test_data = pd.concat(
        bivariate.merge(chunk, on='SNP') for chrom in range(22, 23)
        for chunk in pd.read_table('/scratch/midway2/aksarkar/deepldsc/baseline/baseline.{}.l2.ldscore.gz'.format(chrom), chunksize=1000)
    )
    training_data.shape, test_data.shape
  #+END_SRC

  #+RESULTS: bivariate-sumstats

  Suppose we actually fit a multi-task model:

  #+NAME: multi-deepldsc
  #+BEGIN_SRC ipython :session kernel-aksarkar.json :results raw drawer :async t
    multi_deepldsc = keras.models.Sequential([
      keras.layers.Dense(32, input_dim=53, activation='relu'),
      keras.layers.Dense(16, activation='relu'),
      keras.layers.Dense(2)
    ])
    multi_deepldsc.compile(optimizer=keras.optimizers.RMSprop(lr=1e-6, clipnorm=1.0),
                           loss='msle')
    multi_deepldsc.fit(x=training_data.iloc[:,-53:].values,
                       y=training_data[['CHISQ_x', 'CHISQ_y']].values,
                       epochs=4,
    )
    multi_deepldsc.save(os.path.join(os.getenv('SCRATCH'), 'deepldsc', 'uc-ibd-deepldsc.hdf5'))
    multi_deepldsc.evaluate(x=test_data.iloc[:,-53:].values,
                            y=test_data[['CHISQ_x', 'CHISQ_y']].values,
    )
  #+END_SRC

  #+RESULTS: multi-deepldsc
  :RESULTS:
  : 0.78156761889424764
  :END:
